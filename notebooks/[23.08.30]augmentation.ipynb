{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== System Information ==========\n",
      "DATE : 2023-08-31\n",
      "Pyton Version : 3.8.17\n",
      "PyTorch Version : 2.0.1\n",
      "OS : Linux 5.4.0-155-generic\n",
      "CPU spec : x86_64\n",
      "RAM spec : 503.73 GB\n",
      "Device 0:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 1:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 2:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 3:\n",
      "Name: NVIDIA DGX Display\n",
      "Total Memory: 3911.875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 4:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from src.tools.print_sysinfo import print_env\n",
    "print_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from src.tools.rle_encoder import rle_encode\n",
    "from src.data.dataset import SourceDataset, TargetDataset\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "\n",
    "def fisheye_circular_transform_torch(image, mask=None, fov_degree=200, focal_scale=4.5):\n",
    "    _, h, w = image.shape\n",
    "    \n",
    "    # Convert degrees to radians using torch tensor\n",
    "    radian_conversion = torch.tensor(np.pi/180, dtype=image.dtype, device=image.device)\n",
    "    \n",
    "    \n",
    "    # Calculate the focal length using the given FOV\n",
    "    f = w / (2 * torch.tan(0.5 * fov_degree * radian_conversion))\n",
    "    f_scaled = f * focal_scale\n",
    "    \n",
    "    # Meshgrid for coordinates\n",
    "    x = torch.linspace(-w//2, w//2, w).repeat(h, 1)\n",
    "    y = torch.linspace(-h//2, h//2, h).unsqueeze(1).repeat(1, w)\n",
    "    r = torch.sqrt(x*x + y*y)\n",
    "    theta = torch.atan2(y, x)\n",
    "    \n",
    "    # Apply fisheye transformation\n",
    "    r_fisheye = f_scaled * torch.atan(r / f_scaled)\n",
    "    x_fisheye = (w // 2 + r_fisheye * torch.cos(theta)).long()\n",
    "    y_fisheye = (h // 2 + r_fisheye * torch.sin(theta)).long()\n",
    "    \n",
    "    # Create masks for valid coordinates\n",
    "    valid_coords = (x_fisheye >= 0) & (x_fisheye < w) & (y_fisheye >= 0) & (y_fisheye < h)\n",
    "    \n",
    "    # Initialize output images\n",
    "    new_image = torch.zeros_like(image)\n",
    "    if mask is not None:\n",
    "        new_mask = torch.zeros_like(mask)\n",
    "    else:\n",
    "        new_mask = None\n",
    "    \n",
    "    # Assign values\n",
    "    new_image[:, valid_coords] = image[:, y_fisheye[valid_coords], x_fisheye[valid_coords]]\n",
    "    if mask is not None:\n",
    "        new_mask[:, valid_coords] = mask[:, y_fisheye[valid_coords], x_fisheye[valid_coords]]\n",
    "    \n",
    "    return new_image, new_mask\n",
    "\n",
    "class FisheyeTransform(ImageOnlyTransform):\n",
    "    def __init__(self, fov_degree=200, focal_scale=4.5, always_apply=False, p=1.0):\n",
    "        super(FisheyeTransform, self).__init__(always_apply, p)\n",
    "        self.fov_degree = fov_degree\n",
    "        self.focal_scale = focal_scale\n",
    "\n",
    "    def apply(self, image, **params):\n",
    "        image_tensor = torch.tensor(image).permute(2, 0, 1).float()\n",
    "        transformed_image, _ = fisheye_circular_transform_torch(image_tensor, fov_degree=self.fov_degree, focal_scale=self.focal_scale)\n",
    "        return transformed_image.permute(1, 2, 0).byte().numpy()\n",
    "\n",
    "    def apply_to_mask(self, mask, **params):\n",
    "        mask_tensor = torch.tensor(mask).unsqueeze(0).float()\n",
    "        _, transformed_mask = fisheye_circular_transform_torch(mask_tensor, fov_degree=self.fov_degree, focal_scale=self.focal_scale)\n",
    "        return transformed_mask.squeeze(0).byte().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "     GaussNoise, MotionBlur, MedianBlur, PiecewiseAffine,\n",
    "    Sharpen, Emboss, RandomBrightnessContrast, Flip, OneOf, Compose\n",
    ")\n",
    "from albumentations.core.transforms_interface import DualTransform\n",
    "\n",
    "# Here's the fisheye_transform function:\n",
    "def fisheye_transform(image, k):\n",
    "    height, width = image.shape[:2]\n",
    "    fx, fy = width / 2, height / 2\n",
    "\n",
    "    # Generate fisheye corrected image\n",
    "    corrected_image = np.zeros_like(image)\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            theta = np.arctan2(i - fy, j - fx)\n",
    "            r = np.sqrt((i - fy) ** 2 + (j - fx) ** 2)\n",
    "            r_corrected = r / (1 + k * r ** 2)\n",
    "            i_corrected = int(fy + r_corrected * np.sin(theta))\n",
    "            j_corrected = int(fx + r_corrected * np.cos(theta))\n",
    "            \n",
    "            # Ensure new coordinates are within image bounds\n",
    "            if 0 <= i_corrected < height and 0 <= j_corrected < width:\n",
    "                corrected_image[i, j] = image[i_corrected, j_corrected]\n",
    "    return corrected_image\n",
    "\n",
    "class FisheyeAug(A.DualTransform):\n",
    "    def __init__(self, k=0.5, p=0.5):\n",
    "        super(FisheyeAug, self).__init__(p=p)\n",
    "        self.k = k\n",
    "\n",
    "    def apply(self, img, **params):\n",
    "        return fisheye_transform(img, self.k)\n",
    "    \n",
    "    def apply_to_mask(self, mask, **params):\n",
    "        return fisheye_transform(mask, self.k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(always_apply=True),\n",
    "        OneOf([\n",
    "            GaussNoise(always_apply=True),\n",
    "        ], p=0.2),\n",
    "        OneOf([\n",
    "            MotionBlur(p=0.2),\n",
    "            MedianBlur(blur_limit=3, p=0.1, always_apply=True),\n",
    "            Blur(blur_limit=3, p=0.1, always_apply=True),\n",
    "        ], p=0.2),\n",
    "        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=0.2),\n",
    "        OneOf([\n",
    "            OpticalDistortion(p=0.3),\n",
    "            GridDistortion(p=0.1),\n",
    "            PiecewiseAffine(p=0.3),\n",
    "        ], p=0.2),\n",
    "        OneOf([\n",
    "            Sharpen(always_apply=True, p=1.0),\n",
    "            Emboss(always_apply=True, p=1.0),\n",
    "            RandomBrightnessContrast(always_apply=True, p=1.0),\n",
    "        ], p=0.3),\n",
    "        HueSaturationValue(always_apply=True, p=1.0),\n",
    "        FisheyeAug(k=0.5, p=1.0),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    "    return Compose(train_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = A.Compose(\n",
    "    [\n",
    "        FisheyeTransform(p=0.2),\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = A.Compose(\n",
    "    [   \n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n",
    "#augmentation = get_training_augmentation()\n",
    "\n",
    "\n",
    "train_dataset = SourceDataset(csv_file='train_source.csv', transform=augmentation, is_training=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "valid_dataset = SourceDataset(csv_file='val_source.csv', transform=transform, is_training=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class FPN_UNet_Dropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FPN_UNet_Dropout, self).__init__()\n",
    "\n",
    "        # Encoder (Downsampling path)\n",
    "        self.dconv_down1 = double_conv(3, 64)\n",
    "        self.dropout1 = nn.Dropout(0.5)  # 추가된 Dropout 레이어\n",
    "        self.dconv_down2 = double_conv(64, 128)\n",
    "        self.dropout2 = nn.Dropout(0.5)  # 추가된 Dropout 레이어\n",
    "        self.dconv_down3 = double_conv(128, 256)\n",
    "        self.dropout3 = nn.Dropout(0.5)  # 추가된 Dropout 레이어\n",
    "        self.dconv_down4 = double_conv(256, 512)\n",
    "        self.dropout4 = nn.Dropout(0.5)  # 추가된 Dropout 레이어\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Upward path and lateral connections for FPN\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.lateral3 = nn.Conv2d(256, 256, kernel_size=1)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.lateral2 = nn.Conv2d(128, 128, kernel_size=1)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.lateral1 = nn.Conv2d(64, 64, kernel_size=1)\n",
    "        \n",
    "        # FPN heads for each pyramid level\n",
    "        self.fpn_out3 = nn.Conv2d(256, 13, kernel_size=3, padding=1)\n",
    "        self.fpn_out2 = nn.Conv2d(128, 13, kernel_size=3, padding=1)\n",
    "        self.fpn_out1 = nn.Conv2d(64, 13, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(self.dropout1(conv1))  # Dropout 적용\n",
    "        \n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(self.dropout2(conv2))  # Dropout 적용\n",
    "        \n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(self.dropout3(conv3))  # Dropout 적용\n",
    "        \n",
    "        x = self.dropout4(self.dconv_down4(x))  # Dropout 적용\n",
    "\n",
    "        \n",
    "        # Upward path with lateral connections\n",
    "        x = self.upconv3(x)\n",
    "        conv3 = self.lateral3(conv3)\n",
    "        p3 = torch.add(x, conv3)  # Element-wise addition\n",
    "        out3 = self.fpn_out3(p3)\n",
    "        \n",
    "        x = self.upconv2(p3)\n",
    "        conv2 = self.lateral2(conv2)\n",
    "        p2 = torch.add(x, conv2)\n",
    "        out2 = self.fpn_out2(p2)\n",
    "        \n",
    "        x = self.upconv1(p2)\n",
    "        conv1 = self.lateral1(conv1)\n",
    "        p1 = torch.add(x, conv1)\n",
    "        out1 = self.fpn_out1(p1)\n",
    "        \n",
    "        # Note: You can return combined results or individual FPN layer outputs based on the use case.\n",
    "        return out1, out2, out3\n",
    "\n",
    "class FPN_UNet_FC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FPN_UNet_FC, self).__init__()\n",
    "        self.fpn_unet = FPN_UNet_Dropout()\n",
    "        self.upsample = nn.Upsample(size=(224, 224), mode='bilinear', align_corners=True)\n",
    "        self.conv1x1 = nn.Conv2d(13+13+13, 13, kernel_size=1)  # Assuming we're concatenating\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1, out2, out3 = self.fpn_unet(x)\n",
    "\n",
    "        # Upsample each output to the desired size: 224x224\n",
    "        out1_upsampled = self.upsample(out1)\n",
    "        out2_upsampled = self.upsample(out2)\n",
    "        out3_upsampled = self.upsample(out3)\n",
    "\n",
    "        # Concatenate the outputs along the channel dimension\n",
    "        merged_output = torch.cat([out1_upsampled, out2_upsampled, out3_upsampled], dim=1)\n",
    "\n",
    "        # Map to desired number of channels using 1x1 convolution\n",
    "        final_output = self.conv1x1(merged_output)\n",
    "\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_iou(pred, target, num_classes):\n",
    "    iou_list = []\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "\n",
    "    # For classes excluding the background\n",
    "    for cls in range(num_classes - 1):  # We subtract 1 to exclude the background class\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds[target_inds]).sum().float()\n",
    "        union = (pred_inds + target_inds).sum().float()\n",
    "        if union == 0:\n",
    "            iou_list.append(float('nan'))  # If there is no ground truth, do not include in evaluation\n",
    "        else:\n",
    "            iou_list.append((intersection / union).item())\n",
    "    return iou_list\n",
    "\n",
    "def compute_mIoU(preds, labels, num_classes=13):\n",
    "    iou_list = compute_iou(preds, labels, num_classes)\n",
    "    valid_iou_list = [iou for iou in iou_list if not math.isnan(iou)]\n",
    "    mIoU = sum(valid_iou_list) / len(valid_iou_list)\n",
    "    return mIoU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "DeferredCudaCallError",
     "evalue": "CUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1682343962757/work/aten/src/ATen/cuda/CUDAContext.cpp\":50, please report a bug to PyTorch. \n\nCUDA call was originally invoked at:\n\n['  File \"/home/shlee/venvs/vision_task/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\\n    return _run_code(code, main_globals, None,\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/runpy.py\", line 87, in _run_code\\n    exec(code, run_globals)\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\\n    app.launch_new_instance()\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\\n    app.start()\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 736, in start\\n    self.io_loop.start()\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 195, in start\\n    self.asyncio_loop.run_forever()\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\\n    self._run_once()\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\\n    handle._run()\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/asyncio/events.py\", line 81, in _run\\n    self._context.run(self._callback, *self._args)\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\\n    await self.process_one()\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\\n    await dispatch(*args)\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\\n    await result\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\\n    reply_content = await reply_content\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\\n    res = shell.run_cell(\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\\n    return super().run_cell(*args, **kwargs)\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\\n    result = self._run_cell(\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\\n    result = runner(coro)\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\\n    coro.send(None)\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\\n    if await self.run_code(code, result, async_=asy):\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\\n    exec(code_obj, self.user_global_ns, self.user_ns)\\n', '  File \"/tmp/ipykernel_2391470/776561273.py\", line 5, in <module>\\n    print_env()\\n', '  File \"/home/shlee/shlee/samsung_challenge/notebooks/../src/tools/print_sysinfo.py\", line 23, in print_env\\n    import torch\\n', '  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\\n', '  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\\n', '  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\\n', '  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\\n', '  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/torch/__init__.py\", line 1146, in <module>\\n    _C._initExtension(manager_path())\\n', '  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\\n', '  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\\n', '  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\\n', '  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\\n', '  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 197, in <module>\\n    _lazy_call(_check_capability)\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 195, in _lazy_call\\n    _queued_calls.append((callable, traceback.format_stack()))\\n']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/venvs/vision_task/lib/python3.8/site-packages/torch/cuda/__init__.py:260\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     queued_call()\n\u001b[1;32m    261\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/venvs/vision_task/lib/python3.8/site-packages/torch/cuda/__init__.py:145\u001b[0m, in \u001b[0;36m_check_capability\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(device_count()):\n\u001b[0;32m--> 145\u001b[0m     capability \u001b[39m=\u001b[39m get_device_capability(d)\n\u001b[1;32m    146\u001b[0m     major \u001b[39m=\u001b[39m capability[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/venvs/vision_task/lib/python3.8/site-packages/torch/cuda/__init__.py:381\u001b[0m, in \u001b[0;36mget_device_capability\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Gets the cuda capability of a device.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \n\u001b[1;32m    371\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39m    tuple(int, int): the major and minor cuda capability of the device\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 381\u001b[0m prop \u001b[39m=\u001b[39m get_device_properties(device)\n\u001b[1;32m    382\u001b[0m \u001b[39mreturn\u001b[39;00m prop\u001b[39m.\u001b[39mmajor, prop\u001b[39m.\u001b[39mminor\n",
      "File \u001b[0;32m~/venvs/vision_task/lib/python3.8/site-packages/torch/cuda/__init__.py:399\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid device id\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 399\u001b[0m \u001b[39mreturn\u001b[39;00m _get_device_properties(device)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1682343962757/work/aten/src/ATen/cuda/CUDAContext.cpp\":50, please report a bug to PyTorch. ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDeferredCudaCallError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m FPN_UNet_FC()\n\u001b[1;32m      4\u001b[0m \u001b[39m# if torch.cuda.device_count() > 1:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m#     print(f\"Using {torch.cuda.device_count()} GPUs!\")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m#     model = nn.DataParallel(model)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# else:\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m#     print(f\"Using CPU\")\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m model\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     11\u001b[0m \u001b[39m# 2. 데이터 준비 (여기서는 간략하게 표현합니다)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m#train_loader, val_loader = prepare_target_domain_dataloaders()\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[39m# 3. 학습 설정\u001b[39;00m\n\u001b[1;32m     15\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss() \u001b[39m# 예시로 CrossEntropyLoss를 사용합니다\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/vision_task/lib/python3.8/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/venvs/vision_task/lib/python3.8/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/vision_task/lib/python3.8/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/vision_task/lib/python3.8/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/vision_task/lib/python3.8/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/venvs/vision_task/lib/python3.8/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "File \u001b[0;32m~/venvs/vision_task/lib/python3.8/site-packages/torch/cuda/__init__.py:264\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    262\u001b[0m             msg \u001b[39m=\u001b[39m (\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCUDA call failed lazily at initialization with error: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    263\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCUDA call was originally invoked at:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00morig_traceback\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 264\u001b[0m             \u001b[39mraise\u001b[39;00m DeferredCudaCallError(msg) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     \u001b[39mdelattr\u001b[39m(_tls, \u001b[39m'\u001b[39m\u001b[39mis_initializing\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mDeferredCudaCallError\u001b[0m: CUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1682343962757/work/aten/src/ATen/cuda/CUDAContext.cpp\":50, please report a bug to PyTorch. \n\nCUDA call was originally invoked at:\n\n['  File \"/home/shlee/venvs/vision_task/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\\n    return _run_code(code, main_globals, None,\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/runpy.py\", line 87, in _run_code\\n    exec(code, run_globals)\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\\n    app.launch_new_instance()\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\\n    app.start()\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 736, in start\\n    self.io_loop.start()\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 195, in start\\n    self.asyncio_loop.run_forever()\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\\n    self._run_once()\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\\n    handle._run()\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/asyncio/events.py\", line 81, in _run\\n    self._context.run(self._callback, *self._args)\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\\n    await self.process_one()\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\\n    await dispatch(*args)\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\\n    await result\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\\n    reply_content = await reply_content\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\\n    res = shell.run_cell(\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\\n    return super().run_cell(*args, **kwargs)\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\\n    result = self._run_cell(\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\\n    result = runner(coro)\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\\n    coro.send(None)\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\\n    if await self.run_code(code, result, async_=asy):\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\\n    exec(code_obj, self.user_global_ns, self.user_ns)\\n', '  File \"/tmp/ipykernel_2391470/776561273.py\", line 5, in <module>\\n    print_env()\\n', '  File \"/home/shlee/shlee/samsung_challenge/notebooks/../src/tools/print_sysinfo.py\", line 23, in print_env\\n    import torch\\n', '  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\\n', '  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\\n', '  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\\n', '  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\\n', '  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/torch/__init__.py\", line 1146, in <module>\\n    _C._initExtension(manager_path())\\n', '  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\\n', '  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\\n', '  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\\n', '  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\\n', '  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 197, in <module>\\n    _lazy_call(_check_capability)\\n', '  File \"/home/shlee/venvs/vision_task/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 195, in _lazy_call\\n    _queued_calls.append((callable, traceback.format_stack()))\\n']"
     ]
    }
   ],
   "source": [
    "# 1. 모델 불러오기\n",
    "model = FPN_UNet_FC()\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "#     model = nn.DataParallel(model)\n",
    "# else:\n",
    "#     print(f\"Using CPU\")\n",
    "model.to(device)\n",
    "\n",
    "# 2. 데이터 준비 (여기서는 간략하게 표현합니다)\n",
    "#train_loader, val_loader = prepare_target_domain_dataloaders()\n",
    "\n",
    "# 3. 학습 설정\n",
    "criterion = nn.CrossEntropyLoss() # 예시로 CrossEntropyLoss를 사용합니다\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) # 작은 learning rate 사용\n",
    "\n",
    "# 4. 학습\n",
    "num_epochs = 1000\n",
    "train_losses = []\n",
    "train_mIoUs = []\n",
    "val_mIoUs = []\n",
    "\n",
    "# Early stopping 관련 설정\n",
    "patience = 10  # 10번의 epoch 동안 성능 향상이 없을 경우 학습 중단\n",
    "no_improve_epochs = 0  # 성능 향상이 없는 epoch의 횟수\n",
    "best_mIoU = 0.0  # 최고의 검증 mIoU 저장\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_iou = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for images, masks in tqdm(train_loader):\n",
    "        images = images.float().to(device)\n",
    "        masks = masks.long().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total_iou += compute_mIoU(predicted, masks)\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_train_mIoU = total_iou / num_batches\n",
    "    train_losses.append(avg_loss)\n",
    "    train_mIoUs.append(avg_train_mIoU)\n",
    "    print(f\"Epoch {epoch + 1} - Training Loss: {avg_loss:.4f}, Training mIoU: {avg_train_mIoU:.4f}\")\n",
    "\n",
    "    \n",
    "    # 5. 검증 (간략하게 표현)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_iou = 0\n",
    "        num_images = 0\n",
    "        for images, masks in tqdm(valid_loader):\n",
    "            images = images.float().to(device)\n",
    "            masks = masks.long().to(device)\n",
    "        \n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_iou += compute_mIoU(predicted, masks)\n",
    "            num_images += images.size(0)\n",
    "        avg_mIoU = total_iou / num_images\n",
    "        print(f\"Epoch {epoch + 1}, mIoU: {avg_mIoU:.4f}\")\n",
    "\n",
    "    # Early stopping 검사\n",
    "    if avg_mIoU > best_mIoU:\n",
    "        best_mIoU = avg_mIoU\n",
    "        # 최적의 모델 저장\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            # 최적의 모델 불러오기\n",
    "            model.load_state_dict(torch.load('best_model.pth'))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision_task",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
