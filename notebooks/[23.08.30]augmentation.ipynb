{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== System Information ==========\n",
      "DATE : 2023-08-30\n",
      "Pyton Version : 3.8.17\n",
      "PyTorch Version : 1.12.1\n",
      "OS : Linux 5.4.0-155-generic\n",
      "CPU spec : x86_64\n",
      "RAM spec : 503.73 GB\n",
      "Device 0:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 1:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 2:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 3:\n",
      "Name: NVIDIA DGX Display\n",
      "Total Memory: 3911.875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 4:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from src.tools.print_sysinfo import print_env\n",
    "print_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shlee/venvs/vision_task/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libc10_cuda.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from src.tools.rle_encoder import rle_encode\n",
    "from src.data.dataset import SourceDataset, TargetDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 'No GPU available')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU 사용 가능 여부 확인\n",
    "gpu_available = torch.cuda.is_available()\n",
    "\n",
    "# GPU 이름 가져오기\n",
    "gpu_name = torch.cuda.get_device_name(0) if gpu_available else \"No GPU available\"\n",
    "\n",
    "gpu_available, gpu_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import functional as F\n",
    "\n",
    "class FisheyeTransform:\n",
    "    def __init__(self, k=0.5, center=None):\n",
    "        \"\"\"\n",
    "        Initialize the transform with distortion coefficient and center.\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.center = center\n",
    "\n",
    "    def __call__(self, image):\n",
    "        \"\"\"\n",
    "        Apply the fisheye transform.\n",
    "        \"\"\"\n",
    "        image = np.array(image)\n",
    "        transformed_image = self.fisheye_transform(image, self.k, self.center)\n",
    "        return F.to_pil_image(transformed_image)\n",
    "\n",
    "    def fisheye_transform(self, image, k, center):\n",
    "        \"\"\"\n",
    "        Apply fisheye transformation to an image.\n",
    "        \"\"\"\n",
    "        rows, cols, _ = image.shape\n",
    "        if center is None:\n",
    "            center = (cols // 2, rows // 2)\n",
    "\n",
    "        map_x = np.zeros((rows, cols), dtype=np.float32)\n",
    "        map_y = np.zeros((rows, cols), dtype=np.float32)\n",
    "\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                r = np.sqrt((i - center[1]) ** 2 + (j - center[0]) ** 2)\n",
    "                theta = np.arctan(r)\n",
    "                theta_d = theta + k * theta ** 3\n",
    "                scale = theta_d / r if r != 0 else 1\n",
    "                map_x[i, j] = center[0] + (j - center[0]) * scale\n",
    "                map_y[i, j] = center[1] + (i - center[1]) * scale\n",
    "\n",
    "        return cv2.remap(image, map_x, map_y, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "     GaussNoise, MotionBlur, MedianBlur, PiecewiseAffine,\n",
    "    Sharpen, Emboss, RandomBrightnessContrast, Flip, OneOf, Compose\n",
    ")\n",
    "from albumentations.core.transforms_interface import DualTransform\n",
    "\n",
    "# Here's the fisheye_transform function:\n",
    "def fisheye_transform(image, k):\n",
    "    height, width = image.shape[:2]\n",
    "    fx, fy = width / 2, height / 2\n",
    "\n",
    "    # Generate fisheye corrected image\n",
    "    corrected_image = np.zeros_like(image)\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            theta = np.arctan2(i - fy, j - fx)\n",
    "            r = np.sqrt((i - fy) ** 2 + (j - fx) ** 2)\n",
    "            r_corrected = r / (1 + k * r ** 2)\n",
    "            i_corrected = int(fy + r_corrected * np.sin(theta))\n",
    "            j_corrected = int(fx + r_corrected * np.cos(theta))\n",
    "            \n",
    "            # Ensure new coordinates are within image bounds\n",
    "            if 0 <= i_corrected < height and 0 <= j_corrected < width:\n",
    "                corrected_image[i, j] = image[i_corrected, j_corrected]\n",
    "    return corrected_image\n",
    "\n",
    "class FisheyeAug(A.DualTransform):\n",
    "    def __init__(self, k=0.5, p=0.5):\n",
    "        super(FisheyeAug, self).__init__(p=p)\n",
    "        self.k = k\n",
    "\n",
    "    def apply(self, img, **params):\n",
    "        return fisheye_transform(img, self.k)\n",
    "    \n",
    "    def apply_to_mask(self, mask, **params):\n",
    "        return fisheye_transform(mask, self.k)\n",
    "    \n",
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(always_apply=True),\n",
    "        OneOf([\n",
    "            GaussNoise(always_apply=True),\n",
    "        ], p=0.2),\n",
    "        OneOf([\n",
    "            MotionBlur(p=0.2),\n",
    "            MedianBlur(blur_limit=3, p=0.1, always_apply=True),\n",
    "            Blur(blur_limit=3, p=0.1, always_apply=True),\n",
    "        ], p=0.2),\n",
    "        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=0.2),\n",
    "        OneOf([\n",
    "            OpticalDistortion(p=0.3),\n",
    "            GridDistortion(p=0.1),\n",
    "            PiecewiseAffine(p=0.3),\n",
    "        ], p=0.2),\n",
    "        OneOf([\n",
    "            Sharpen(always_apply=True, p=1.0),\n",
    "            Emboss(always_apply=True, p=1.0),\n",
    "            RandomBrightnessContrast(always_apply=True, p=1.0),\n",
    "        ], p=0.3),\n",
    "        HueSaturationValue(always_apply=True, p=1.0),\n",
    "        FisheyeAug(k=0.5, p=1.0),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    "    return Compose(train_transform)\n",
    "\n",
    "transform = A.Compose(\n",
    "    [   \n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n",
    "augmentation = get_training_augmentation()\n",
    "\n",
    "\n",
    "train_dataset = SourceDataset(csv_file='train_source.csv', transform=augmentation, is_training=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "valid_dataset = SourceDataset(csv_file='val_source.csv', transform=transform, is_training=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class FPN_UNet_Dropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FPN_UNet_Dropout, self).__init__()\n",
    "\n",
    "        # Encoder (Downsampling path)\n",
    "        self.dconv_down1 = double_conv(3, 64)\n",
    "        self.dropout1 = nn.Dropout(0.5)  # 추가된 Dropout 레이어\n",
    "        self.dconv_down2 = double_conv(64, 128)\n",
    "        self.dropout2 = nn.Dropout(0.5)  # 추가된 Dropout 레이어\n",
    "        self.dconv_down3 = double_conv(128, 256)\n",
    "        self.dropout3 = nn.Dropout(0.5)  # 추가된 Dropout 레이어\n",
    "        self.dconv_down4 = double_conv(256, 512)\n",
    "        self.dropout4 = nn.Dropout(0.5)  # 추가된 Dropout 레이어\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Upward path and lateral connections for FPN\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.lateral3 = nn.Conv2d(256, 256, kernel_size=1)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.lateral2 = nn.Conv2d(128, 128, kernel_size=1)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.lateral1 = nn.Conv2d(64, 64, kernel_size=1)\n",
    "        \n",
    "        # FPN heads for each pyramid level\n",
    "        self.fpn_out3 = nn.Conv2d(256, 13, kernel_size=3, padding=1)\n",
    "        self.fpn_out2 = nn.Conv2d(128, 13, kernel_size=3, padding=1)\n",
    "        self.fpn_out1 = nn.Conv2d(64, 13, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(self.dropout1(conv1))  # Dropout 적용\n",
    "        \n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(self.dropout2(conv2))  # Dropout 적용\n",
    "        \n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(self.dropout3(conv3))  # Dropout 적용\n",
    "        \n",
    "        x = self.dropout4(self.dconv_down4(x))  # Dropout 적용\n",
    "\n",
    "        \n",
    "        # Upward path with lateral connections\n",
    "        x = self.upconv3(x)\n",
    "        conv3 = self.lateral3(conv3)\n",
    "        p3 = torch.add(x, conv3)  # Element-wise addition\n",
    "        out3 = self.fpn_out3(p3)\n",
    "        \n",
    "        x = self.upconv2(p3)\n",
    "        conv2 = self.lateral2(conv2)\n",
    "        p2 = torch.add(x, conv2)\n",
    "        out2 = self.fpn_out2(p2)\n",
    "        \n",
    "        x = self.upconv1(p2)\n",
    "        conv1 = self.lateral1(conv1)\n",
    "        p1 = torch.add(x, conv1)\n",
    "        out1 = self.fpn_out1(p1)\n",
    "        \n",
    "        # Note: You can return combined results or individual FPN layer outputs based on the use case.\n",
    "        return out1, out2, out3\n",
    "\n",
    "class FPN_UNet_FC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FPN_UNet_FC, self).__init__()\n",
    "        self.fpn_unet = FPN_UNet_Dropout()\n",
    "        self.upsample = nn.Upsample(size=(224, 224), mode='bilinear', align_corners=True)\n",
    "        self.conv1x1 = nn.Conv2d(13+13+13, 13, kernel_size=1)  # Assuming we're concatenating\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1, out2, out3 = self.fpn_unet(x)\n",
    "\n",
    "        # Upsample each output to the desired size: 224x224\n",
    "        out1_upsampled = self.upsample(out1)\n",
    "        out2_upsampled = self.upsample(out2)\n",
    "        out3_upsampled = self.upsample(out3)\n",
    "\n",
    "        # Concatenate the outputs along the channel dimension\n",
    "        merged_output = torch.cat([out1_upsampled, out2_upsampled, out3_upsampled], dim=1)\n",
    "\n",
    "        # Map to desired number of channels using 1x1 convolution\n",
    "        final_output = self.conv1x1(merged_output)\n",
    "\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_iou(pred, target, num_classes):\n",
    "    iou_list = []\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "\n",
    "    # For classes excluding the background\n",
    "    for cls in range(num_classes - 1):  # We subtract 1 to exclude the background class\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds[target_inds]).sum().float()\n",
    "        union = (pred_inds + target_inds).sum().float()\n",
    "        if union == 0:\n",
    "            iou_list.append(float('nan'))  # If there is no ground truth, do not include in evaluation\n",
    "        else:\n",
    "            iou_list.append((intersection / union).item())\n",
    "    return iou_list\n",
    "\n",
    "def compute_mIoU(preds, labels, num_classes=13):\n",
    "    iou_list = compute_iou(preds, labels, num_classes)\n",
    "    valid_iou_list = [iou for iou in iou_list if not math.isnan(iou)]\n",
    "    mIoU = sum(valid_iou_list) / len(valid_iou_list)\n",
    "    return mIoU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/138 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# 1. 모델 불러오기\n",
    "model = FPN_UNet_FC()\n",
    "#model.load_state_dict(torch.load('path_to_pretrained_model.pth'))\n",
    "model.to(device)\n",
    "\n",
    "# 2. 데이터 준비 (여기서는 간략하게 표현합니다)\n",
    "#train_loader, val_loader = prepare_target_domain_dataloaders()\n",
    "\n",
    "# 3. 학습 설정\n",
    "criterion = nn.CrossEntropyLoss() # 예시로 CrossEntropyLoss를 사용합니다\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) # 작은 learning rate 사용\n",
    "\n",
    "# 4. 학습\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "train_mIoUs = []\n",
    "val_mIoUs = []\n",
    "\n",
    "# Early stopping 관련 설정\n",
    "patience = 10  # 10번의 epoch 동안 성능 향상이 없을 경우 학습 중단\n",
    "no_improve_epochs = 0  # 성능 향상이 없는 epoch의 횟수\n",
    "best_mIoU = 0.0  # 최고의 검증 mIoU 저장\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_iou = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for images, masks in tqdm(train_loader):\n",
    "        images = images.float().to(device)\n",
    "        masks = masks.long().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total_iou += compute_mIoU(predicted, masks)\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_train_mIoU = total_iou / num_batches\n",
    "    train_losses.append(avg_loss)\n",
    "    train_mIoUs.append(avg_train_mIoU)\n",
    "    print(f\"Epoch {epoch + 1} - Training Loss: {avg_loss:.4f}, Training mIoU: {avg_train_mIoU:.4f}\")\n",
    "\n",
    "    \n",
    "    # 5. 검증 (간략하게 표현)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_iou = 0\n",
    "        num_images = 0\n",
    "        for images, masks in tqdm(valid_loader):\n",
    "            images = images.float().to(device)\n",
    "            masks = masks.long().to(device)\n",
    "        \n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_iou += compute_mIoU(predicted, masks)\n",
    "            num_images += images.size(0)\n",
    "        avg_mIoU = total_iou / num_images\n",
    "        print(f\"Epoch {epoch + 1}, mIoU: {avg_mIoU:.4f}\")\n",
    "\n",
    "    # Early stopping 검사\n",
    "    if avg_mIoU > best_mIoU:\n",
    "        best_mIoU = avg_mIoU\n",
    "        # 최적의 모델 저장\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            # 최적의 모델 불러오기\n",
    "            model.load_state_dict(torch.load('best_model.pth'))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision_task",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
