{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== System Information ==========\n",
      "DATE : 2023-08-31\n",
      "Pyton Version : 3.10.12\n",
      "PyTorch Version : 2.0.1\n",
      "OS : Linux 5.15.0-78-generic\n",
      "CPU spec : x86_64\n",
      "RAM spec : 122.84 GB\n",
      "Device 0:\n",
      "Name: NVIDIA GeForce RTX 3090\n",
      "Total Memory: 24576.0 MB\n",
      "Driver Version: 530.41.03\n",
      "==============================\n",
      "Device 1:\n",
      "Name: NVIDIA GeForce RTX 3090\n",
      "Total Memory: 24576.0 MB\n",
      "Driver Version: 530.41.03\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from src.tools.print_sysinfo import print_env\n",
    "print_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from src.tools.rle_encoder import rle_encode\n",
    "from src.data.dataset import SourceDataset, TargetDataset\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "\n",
    "def fisheye_circular_transform_torch(image, mask=None, fov_degree=200, focal_scale=4.5):\n",
    "    _, h, w = image.shape\n",
    "    \n",
    "    # Convert degrees to radians using torch tensor\n",
    "    radian_conversion = torch.tensor(np.pi/180, dtype=image.dtype, device=image.device)\n",
    "    \n",
    "    \n",
    "    # Calculate the focal length using the given FOV\n",
    "    f = w / (2 * torch.tan(0.5 * fov_degree * radian_conversion))\n",
    "    f_scaled = f * focal_scale\n",
    "    \n",
    "    # Meshgrid for coordinates\n",
    "    x = torch.linspace(-w//2, w//2, w).repeat(h, 1)\n",
    "    y = torch.linspace(-h//2, h//2, h).unsqueeze(1).repeat(1, w)\n",
    "    r = torch.sqrt(x*x + y*y)\n",
    "    theta = torch.atan2(y, x)\n",
    "    \n",
    "    # Apply fisheye transformation\n",
    "    r_fisheye = f_scaled * torch.atan(r / f_scaled)\n",
    "    x_fisheye = (w // 2 + r_fisheye * torch.cos(theta)).long()\n",
    "    y_fisheye = (h // 2 + r_fisheye * torch.sin(theta)).long()\n",
    "    \n",
    "    # Create masks for valid coordinates\n",
    "    valid_coords = (x_fisheye >= 0) & (x_fisheye < w) & (y_fisheye >= 0) & (y_fisheye < h)\n",
    "    \n",
    "    # Initialize output images\n",
    "    new_image = torch.zeros_like(image)\n",
    "    if mask is not None:\n",
    "        new_mask = torch.zeros_like(mask)\n",
    "    else:\n",
    "        new_mask = None\n",
    "    \n",
    "    # Assign values\n",
    "    new_image[:, valid_coords] = image[:, y_fisheye[valid_coords], x_fisheye[valid_coords]]\n",
    "    if mask is not None:\n",
    "        new_mask[:, valid_coords] = mask[:, y_fisheye[valid_coords], x_fisheye[valid_coords]]\n",
    "    \n",
    "    return new_image, new_mask\n",
    "\n",
    "class FisheyeTransform(ImageOnlyTransform):\n",
    "    def __init__(self, fov_degree=200, focal_scale=4.5, always_apply=False, p=1.0):\n",
    "        super(FisheyeTransform, self).__init__(always_apply, p)\n",
    "        self.fov_degree = fov_degree\n",
    "        self.focal_scale = focal_scale\n",
    "\n",
    "    def apply(self, image, **params):\n",
    "        image_tensor = torch.tensor(image).permute(2, 0, 1).float()\n",
    "        transformed_image, _ = fisheye_circular_transform_torch(image_tensor, fov_degree=self.fov_degree, focal_scale=self.focal_scale)\n",
    "        return transformed_image.permute(1, 2, 0).byte().numpy()\n",
    "\n",
    "    def apply_to_mask(self, mask, **params):\n",
    "        mask_tensor = torch.tensor(mask).unsqueeze(0).float()\n",
    "        _, transformed_mask = fisheye_circular_transform_torch(mask_tensor, fov_degree=self.fov_degree, focal_scale=self.focal_scale)\n",
    "        return transformed_mask.squeeze(0).byte().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = A.Compose(\n",
    "    [\n",
    "        FisheyeTransform(p=0.2),\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "transform = A.Compose(\n",
    "    [   \n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataset = SourceDataset(csv_file='train_source.csv', transform=augmentation, is_training=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "valid_dataset = SourceDataset(csv_file='val_source.csv', transform=transform, is_training=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_iou(pred, target, num_classes):\n",
    "    iou_list = []\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "\n",
    "    # For classes excluding the background\n",
    "    for cls in range(num_classes - 1):  # We subtract 1 to exclude the background class\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds[target_inds]).sum().float()\n",
    "        union = (pred_inds + target_inds).sum().float()\n",
    "        if union == 0:\n",
    "            iou_list.append(float('nan'))  # If there is no ground truth, do not include in evaluation\n",
    "        else:\n",
    "            iou_list.append((intersection / union).item())\n",
    "    return iou_list\n",
    "\n",
    "def compute_mIoU(preds, labels, num_classes=13):\n",
    "    iou_list = compute_iou(preds, labels, num_classes)\n",
    "    valid_iou_list = [iou for iou in iou_list if not math.isnan(iou)]\n",
    "    mIoU = sum(valid_iou_list) / len(valid_iou_list)\n",
    "    return mIoU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shlee/miniconda3/envs/vision_task/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/shlee/miniconda3/envs/vision_task/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_MobileNet_V3_Large_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/deeplabv3_mobilenet_v3_large-fc3c493d.pth\" to /home/shlee/.cache/torch/hub/checkpoints/deeplabv3_mobilenet_v3_large-fc3c493d.pth\n",
      "100%|██████████| 42.3M/42.3M [00:00<00:00, 68.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "# DeepLabV3 with MobileNetV3 backbone:\n",
    "\n",
    "from torchvision.models.segmentation import deeplabv3_mobilenet_v3_large\n",
    "model = deeplabv3_mobilenet_v3_large(pretrained=True)\n",
    "model.classifier[4] = nn.Conv2d(256, 13, kernel_size=(1, 1), stride=(1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/69 [00:12<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not collections.OrderedDict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     49\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[0;32m---> 50\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, masks)\n\u001b[1;32m     51\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     52\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/vision_task/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/vision_task/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/miniconda3/envs/vision_task/lib/python3.10/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not collections.OrderedDict"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "# 1. 모델 불러오기\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "else:\n",
    "    print(f\"Using CPU\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# 2. 데이터 준비 (여기서는 간략하게 표현합니다)\n",
    "#train_loader, val_loader = prepare_target_domain_dataloaders()\n",
    "\n",
    "# 3. 학습 설정\n",
    "criterion = nn.CrossEntropyLoss() # 예시로 CrossEntropyLoss를 사용합니다\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # 작은 learning rate 사용\n",
    "\n",
    "# Learning rate scheduler 설정\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "\n",
    "# 4. 학습\n",
    "num_epochs = 1000\n",
    "train_losses = []\n",
    "train_mIoUs = []\n",
    "val_mIoUs = []\n",
    "\n",
    "# Early stopping 관련 설정\n",
    "patience = 20  # 10번의 epoch 동안 성능 향상이 없을 경우 학습 중단\n",
    "no_improve_epochs = 0  # 성능 향상이 없는 epoch의 횟수\n",
    "best_mIoU = 0.0  # 최고의 검증 mIoU 저장\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_iou = 0.0\n",
    "    num_batches = 0\n",
    "    num_images = 0\n",
    "    \n",
    "    for images, masks in tqdm(train_loader):\n",
    "        \n",
    "        images = images.float().to(device)\n",
    "        masks = masks.long().to(device)\n",
    "        num_images += images.size(0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += num_images * loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total_iou += compute_mIoU(predicted, masks)\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_images\n",
    "    avg_train_mIoU = total_iou / num_images\n",
    "    train_losses.append(avg_loss)\n",
    "    train_mIoUs.append(avg_train_mIoU)\n",
    "    print(f\"Epoch {epoch + 1} - Training Loss: {avg_loss:.4f}, Training mIoU: {avg_train_mIoU:.4f}\")\n",
    "\n",
    "    \n",
    "    # 5. 검증 (간략하게 표현)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_iou = 0\n",
    "        num_images = 0\n",
    "        for images, masks in tqdm(valid_loader):\n",
    "            images = images.float().to(device)\n",
    "            masks = masks.long().to(device)\n",
    "        \n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_iou += compute_mIoU(predicted, masks)\n",
    "            num_images += images.size(0)\n",
    "        avg_mIoU = total_iou / num_images\n",
    "        print(f\"Epoch {epoch + 1}, mIoU: {avg_mIoU:.4f}\")\n",
    "\n",
    "\n",
    "    # 학습률 업데이트\n",
    "    scheduler.step()\n",
    "\n",
    "    \n",
    "    # Early stopping 검사\n",
    "    if avg_mIoU > best_mIoU:\n",
    "        best_mIoU = avg_mIoU\n",
    "        # 최적의 모델 저장\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            # 최적의 모델 불러오기\n",
    "            model.load_state_dict(torch.load('best_model.pth'))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision_task",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
